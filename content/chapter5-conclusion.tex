\chapter*{结论}\label{ch:conclusion}
\addcontentsline{toc}{chapter}{结论}

% -------------------------- %
% -------- Summary --------- %
% -------------------------- %
\section*{总结}\label{sec:conclusion}
% -- Related Work -- %
本文对生成式对话领域的模型，数据集和指标进行了一次深入考察。
本文首先介绍了本领域的研究现状，
接着介绍了生成式模型的定义，RNN语言模型和Seq2Seq框架。
本文着重介绍了本领域的常见指标，包括基于词重叠的指标，基于词嵌入的指标，
衡量概率语言模型性能的困惑度，以及专门为生成式对话设计的ADEM和RUBER等等。
最后，本文介绍了本领域常用的开放领域对话数据集。

% -- Methodology -- %
在众多模型，数据集和指标中，本文选择了Serban等人在文献\cite{VHRED}
中使用的三个模型HRED，LSTM和VHRED。
在数据集方面，本文选择了公开的，代表了不同领域的三个数据集，
分别是Ubuntu对话数据集，OpenSubtitles和LSDSCC数据集。
在指标方面，本文尽可能涵盖了对话领域使用过或者提出的指标。
本文的主要工作是：
\begin{enumerate}
    \item 在多个数据集上训练多个模型。
    \item 在训练结果上运行多个评价指标。
    \item 对指标进行多种数据分析。
\end{enumerate}
实验配置与文献\cite{HowNot}大致对齐。
实验数据包括多个模型和数据集的组合在不同指标上的得分，以及这些模型输出的响应。
由于时间关系，本文只分析了得分的数据，把分析模型的输出留给后续工作。

% -- Experiment -- %
本文分析了系统层面得分和句子层面得分。
系统层面得分是是对模型在一个数据集上的表现的粗粒度考察，
它可能掩盖了一些事实，
但是便于综合考察模型、数据集和指标三者的整体关系。
参照文献\cite{HowNot}，本文对指标进行分组，并围绕各组指标进行分析。
各模型的系统层面得分随着数据集的变化和指标的变化有较大差异，
比较稳定的是同一个数据集上各模型的排名，
一般HRED和VHRED比LSTM得分高。
模型在Ubuntu对话数据集上的各项指标通常更好。
除了实验过程本身的噪音外，
主要的原因是开放领域对话数据集的特征变化大，
模型无法将一个数据集上的性能完全迁移到另一个数据集上。

本文还考察了句子层面得分的单变量分布，
从分布图像中发现了指标的集群现象。
集群现象反映了提取相同特征的指标有着相似的分布。
指标分布的图像大致可分为两类：
\begin{enumerate}
    \item 基于词嵌入的指标的图像是钟型曲线，接近高斯分布。
    \item 基于词重叠的指标的图像是不对称的双峰曲线，大量句子集中在均值附近很小的范围。
\end{enumerate}
本文尚不清楚指标的具体算法或者模型是如何影响分布的，有待后续研究。

% -------------------------- %
% ------ Future Work ------- %
% -------------------------- %
\section*{展望}\label{sec:future_work}
在实验结论的基础上，
本文对生成式对话领域的模型，数据集以及指标提出几点展望。

% -- Model -- %
在模型方面，尽管Seq2Seq框架在机器翻译领域取得成功，
但是在更加困难的对话响应生成领域，它的表达力遇到了瓶颈。
因此，本文建议尝试新的模型体系结构。
Transformer框架\upcite{Transformer}是一种在大规模语料库上预训练的，
能根据特定任务微调的语言模型。
基于Transformer框架的模型在多项自然语言处理任务中取得了目前最高水平，
将其应用到生成式对话领域有望提升模型的性能。
其他值得尝试的体系结构有对抗生成网络和强化学习，
Li等人在这方面获得了意义重大的进展\upcite{deep_RL,Adversarial}。
另一方面，本文建议在模型中添加额外的特征，比如感情色彩\upcite{ECM}，
主题词\upcite{Topic_Aware}和对话者身份信息\upcite{persona}等等。
添加额外特征能使模型的输出在这些特征上具有一致性，从而提高人类评价的得分。

% -- Dataset -- %
在数据集方面，通过在线聊天收集的对话样本具有很高的多样性，
这和人类在这种开放和匿名的平台的表现有密切联系。
从概率的角度，数据集的样本分布可以看作是非常多个随机变量的叠加，
如果这些随机变量都是独立同分布的话，
整个数据集的样本分布就趋向于高斯分布。
面对复杂的数据集，本文建议使用概率统计工具分析它在
多个方面的分布情况，例如情感分布，对话轮数分布等等。
了解数据集的统计特征有助于理解在这个数据集上训练模型的难度。

% -- Metric -- %
在指标方面，本文的实验结论补充了文献\cite{HowNot}的结论。
为了深入理解对话生成问题，本文提出一个问题：什么样的对话才是好的对话？
这是一个重要的问题，它不仅指导着指标的构建，还决定了模型优化的方向。
人类的对话是在自然环境和社会环境中进化出的一种语言现象，
它根据不同场合和参与者的变化而变化的，非常复杂。
所以这个问题可能难以回答，甚至没有普适性的答案，

从实用的角度，
可以通过实验发现在某些数据集上好的对话应有的特征。
直观的来说，
在Ubuntu对话数据集上，对话以“提问-解决问题”为主，
好的对话应该能帮助人们解决问题，
因此应该和具体的技术话题有较高的相关性；
在Twitter对话数据集上，
对话主要是人们发布的个人状态信息，经常带有感情色彩，
而且人们期望从响应中看到相同的话题或者新奇的事物，
因此好的对话应该考虑情感因素，
关注主题的同时又具有一定的多样性，
在LSDSCC数据集上，对话主要是人们发表对电影的点评，
人们一般希望和看过同类电影的人一起讨论，
虽然有时候有人会发表极端的评价，
但是从长远来看，人们不希望总是看到极端的评价，
所以好的对话应该是对相关电影的中肯评价，
并且带有个性化看法。

综上所述，不妨给“什么样的对话才是好的对话”加上“在某个数据集上”的限定词，
从某个数据集中发现好的对话的模式，
然后设计出能捕获这些模式的指标，
并用它评价在该数据集上训练的模型。
具体来说，本文提出把“设计出和人类评价相关度高的指标”这一任务分解为若干个小任务：
\begin{enumerate}
    \item 把问题限制在某个数据集上。
    \item 找出这个数据集上人类评价高的对话所具有的特征。
    \item 设计出能准确捕获这些特征的指标。
    \item 用人类评价验证指标在对应数据集上的有效性。
\end{enumerate}

本文把这个方法称为数据驱动的指标构建法（Data-Driven Metric Construction）。
必须指出，第二步和第三步具有很大的难度。
第二步一般需要人类评价员对数据集的样本打分，
这将导致带有人类评价的样本比较稀少，
要求指标的泛化能力比较强\upcite{ADEM}。
第三步涉及的特征比较抽象，要求指标的建模能力比较强。
