% !Mode:: "TeX:UTF-8"
% Author: Zhengxi Tian
% Email: zhengxi.tian@hotmail.com

\chapter{相关工作}

情感分析的概念，是Pang~\upcite{DBLP:conf/emnlp/PangLV02}在2002年第一次提出的，并受到学术界和工业界的极大关注.
由于现代社交媒体可以通过互联网收集大量的评论信息，近两年，情感分析在分析网上文献信息方面~\upcite{DBLP:journals/tkde/TanLSGYBCH14}也有了很大的发展。
因为具备分析在线文档极性的能力，情感分析已经成为许多应用中一项基本的技术~\upcite{DBLP:journals/csur/YadollahiSZ17}。
为了更好地获得满意的情感分析结果，如何提取文档的特征，构建强大的模型来学习特征已经成为现在研究领域的一项重要研究问题~\upcite{DBLP:conf/lrec/KiritchenkoM16}。

\section{情感分析的计算方法}
基于情感的表达方式，情感分析的计算方法可以划分为两种模型——维度模型（dimensional models）和分类模型（categorical models）~\upcite{DBLP:journals/taffco/CalvoD10}。
维度模型偏重于强调理解情感经历的价态（valence）和觉醒（arousal）的基本维度~\upcite{russell2003core}。
分类模型则涉及使用分类表示，其中，情感由多个标签表示~\upcite{DBLP:journals/ci/GuptaGF13}。
例如，Ekman基本情绪集~\upcite{ekman1992argument}包括愤怒、厌恶、恐惧、幸福、悲伤和惊奇，如表~\ref{scaling}所示的样例形象地进行了阐明。
样例句子是：Trains crash near Thai resort town。
按照Ekman的方法对该句子进行情感注释，并得到每一个单词的分值。

\begin{table}[]
\centering
\caption{Scaling System样例说明}
\label{scaling}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
例句                  & \multicolumn{6}{c}{Trains crash near Thai resort town} \\ \hline
\multirow{2}{*}{情感} & anger  & disgust  & fear  & joy  & sadness  & surprise  \\ \cline{2-7} 
                    & 2      & 0        & 62    & 0    & 90       & 10        \\ \hline
\end{tabular}
\end{table}

确定情感的另外一种计算方法是使用缩放系统（Scaling System）。
这种方法通常将与负面，中性，正面情感相关联的词汇用范围区间在[-10,10]之内的数字表示。
这使得调整一个给定词相对于它的所处环境，尤指其所在的句子的情感成为了可能。
当一个非结构化的文本使用自然语言处理进行分析时，
指定环境中的每个概念都被赋予一个基于情感词及其相关分数的评分~\upcite{DBLP:journals/coling/TaboadaBTVS11}。
Scaling System使得复杂的情感可以得到进一步得到更充分的理解，
因为可以通过单词在它所处句子中的位置和作用，来调整情感分布的权重。
举例来说，词汇的强化，弱化和否定概念所表达的情感可能会影响它的得分。
通常情况下，如果文本的目标是确定文本中的情感，而不是文本的整体极性和强度，
那么文本可以被给予正面和负面的情感强度评分~\upcite{DBLP:journals/jasis/ThelwallBPCK10}。

对文本情分的分类主要分为：二分类，三分类和多分类。
二分类的情感标签是{正向，负向}，三分类的情感标签是{正向，中性，负向}。
多分类则类似于Ekman的方法，将文本从更多种复杂的情感进行分类。

\section{基于计算语言学的情感分析方法}

\subsection{情感词典的建立}
计算语言学的方法是将预先定好的语言信息进行情感分析~\upcite{DBLP:journals/coling/TaboadaBTVS11}。
传统来讲，从文本中提取特征最简单的方法是使用独热表示（one-hot representation），这种方法也被称为词袋（bag-of-words）~\upcite{DBLP:journals/tkde/ZhuangWXWYLZ17}。
每一个单词都可以转换为，只有一个1和许多个0的向量。这种方法非常方便于在代码中进行实现。
基于这种特征，研究者们提出了很多不同的方法，其中，构建情感词典（sentiment lexicon）的方法和传统的机器学习方法（machine learning）是最受欢迎的~\upcite{DBLP:journals/jasis/DiazTM16}。

基于情感词典的方法是使用预定义的情感评分来预测某一文本的整体极性~\upcite{DBLP:conf/lrec/KiritchenkoM16}。
它预先假设文本情感的极性，是其中单词或单元的所有情感分值的总和~\upcite{DBLP:conf/acii/NeviarouskayaPI09}。
在这种方法中，需要预先构建一个包含大量单词或短语的语义信息词典。信息可以从域内和域外知识获得。
所以，情感词典也主要分为两大类别，与领域有关词典和与领域无关词典。
与领域无关词典包含的单词或短语在不同的领域内都具有相同的含义。
在情感词典中，每一句话都有一个情感打分。分数代表某一单词或短语的情感强烈程度。
总体情感极性是通过增加语句中每个单词的分数来计算的。
计算公式定义如下：

\begin{equation}
s = sign\left(\sum_{i=1}^{k}s_{i}+0.5s_{0}\right )
\end{equation}

其中，${s}$表示句子的整体情感分值，${s_{i}}$代表第${i}$个单词的情感分值，而${s_{0}}$是整体情感的主要极性。

除了上面这种在决策过程中通过对句子中的单词或短语情感分值投票的方式，来计算整个句子的极性方法之外，
研究者们还使用整合平均方法~\upcite{DBLP:conf/emnlp/ChoiC08}。这种方法相较于投票方法来说，更为公平。
在整合平均方法中，首先，需要找出每个句子中出现在情感词典中的情感词汇，接下来，对这些情感词汇进行分数求和。
然后，用整体的情感分值之和作分子，句子中在情感词典中有记录的单词个数作分母，取其归一化后的值的符号，即得到句子的情感极性。
如公式~\ref{eq:fairscore}所示，其中，${q_{i}}$表示句子中每个在情感词典中所出现的单词的极性分数，
${N}$表示句子中在情感此单中出现的单词个数，${sign}$则表示符号函数。

\begin{equation}
p = sign\left(\frac{\sum_{i=1}^{N}q_{i}}{N}\right)
\label{eq:fairscore}
\end{equation}

除此之外，也有一些研究者中提出了基于规则反转的计算机制。
在基于规则反转的计算机制中，文本中的语句需要先经过依赖关系分析得到依赖关系树。
接下来，语句的情感极性则可以通过根节点的情感分值和子树的情感分值所计算得到。
该种机制的决策过程体现在依赖关系树的叶节点，最终，根节点的极性可以有子树的极性计算得到。
计算过程如下所示：

\begin{equation}
s_{i}=sign\left ( q_{i}+\sum_{j:h_{j}=i}s_{j}\left ( -1 \right )_{i}^{r} \right )
\end{equation}
\begin{equation}
p=sign\left ( s_{0} +0.5p_{0}\right )
\end{equation}

其中，${s_{i}}$表示根节点为${i}$的子树的情感极性，${q_{i}}$是第${i}$个子句的情感极性。
${r_{i}}$表示如果第${i}$个子句遇到否定词汇，则情感极性反转的指示信号。
${p}$是句子最后的整体情感倾向。

\subsection{计算语言学方法的局限性}
虽然计算语言学方法在自然语言处理，尤其是情感分析领域取得了一些成果，但是它也面临着一些挑战。
第一个是由于其自身缺陷带来的挑战，通常来讲，计算语言学的方法只能给出单词在一般语言环境下的情感打分值。
虽然，一些研究主要对某一个特定领域的情感进行分析，但是，不同的单词在不同的领域范围内情感极性也是不一样的~\upcite{DBLP:conf/aaai/YoshidaHINM11}。
这样来说比较抽象，本文以举例的方式来说明问题。
比如，在“股市（stock market）”领域，“unpredictable”这个单词往往伴随着负向情感倾向。
人们往往使用该词汇表示对股票市场复杂走势，变化多端，难以预测的无奈和焦急。
但是，在生活中十分常见的“电影（movie）”领域，“unpredictable”这个单词往往伴随着正向情感倾向。
本文从公开的Movie Review数据集（烂番茄电影评论数据集）中选取了对电影《母亲！》（\emph Mother!）高度赞扬的影评：

\begin{description}
\item ‘Mother!’ is an unpredictable, unsettling and satisfying work!!!
\end{description}

从中不难看出，采用计算语言学方法所构建出来的情感词典只能在特定语境下给出文本语句的情感倾向。
最好情况也只是判断同一个单词所具有的不同词性的情感分值。
然而，当相同词性的同一个单词放到不同领域和语言环境下，计算语言学方法就无法准确识别出其情感极性。
最直接的想法是，对不同领域建立通用的情感词典。但是，这种依赖通用情感词典的想法在实践中存在一定的困难，因为无法简单的构建一个通用的情感词典~\upcite{DBLP:conf/www/LuCDZ11}。

计算语言学方法所面临的第二个挑战是时代科技进步所带来的挑战。
随着机器学习，尤其是深度学习的飞速发展，高效又多功能的神经网络已经可以做出来卓越级别（state-of-art）的情感预测。
传统的计算语言学方法也渐渐被取而代之，越来越多的学者转向采用机器学习的方法进行情感分析。
关于机器学习和深度学习在情感分析领域中的应用会在本文接下来的部分做详细的介绍。

\section{基于机器学习的情感分析方法}
\subsection{单标签学习方法}
如果考虑将每个基本情感作为句子的类别标签，情感分析可以视为一个分类问题。
学习歧义值（learning ambiguity）是近年来利用机器学习和数据挖掘研究情感分类的热门话题~\upcite{DBLP:journals/tkde/Geng16}。
学习过程本质上是建立从实例（instance）到标签的映射。
在情感分类方面，现今已经有大量相关的研究工作。
通常，有两种标签分配学习方法：单标签学习方法（SLL）和多标签学习方法（MLL）。

单标签学习方法是选择最强的情感作为一句话的情感标签~\upcite{DBLP:conf/cikm/LinH09}，
大多数分类方法还是基于单标签学习~\upcite{DBLP:journals/tois/QuanWZSL15}。

\subsection{多标签学习方法}
多标签学习方法研究的问题都是一个实例同时与一组情感标签相关联，
在过去的十年里，对这种机器学习方法的学习模式研究取得了很大的进展。
虽然多标签学习方法与单标签学习方法的本质区别在于输出空间的标签个数，
但是，因为多标签的引入使得这种学习方法存在三个本质性难点：

1）分类标签的不确定性。有些实例语句中可能只有一个情感标签，但是有些样本中可能有很多个情感标签。

2）由于输出空间的标签的不确定性，标签个数会呈指数型上升，数量庞大。

3）分类标签之间相互依赖。所以多标签学习方法的第三个难题是如何解决标签之间的相互依赖性。

所以，目前学者们往往从两个切入点来进行多标签学习方法的研究，本文将当前主要研究方法分类，
如图~\ref{fig:multilabel}所示。其中第一类是将多标签学习方法转换为分类问题，第二类则是基于算法本身进行适应性更新。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\columnwidth]{figures-atGRU/multilabel.pdf}
	\caption{多标签学习算法的主要分类}
	\label{fig:multilabel}
\end{figure}

正如图~\ref{fig:multilabel}所示，第一类，多标签学习方法可以转换为分类问题。
第二类，可以基于算法本身进行适应性地更新。
问题转换方法（Problem Transformation）的基本思想是对多个标签训练的样本进行预处理，
将多标签学习问题转换为二分类问题，标签排序问题或者多分类问题。这些更细的方法都是现阶段已知并且已做到深入研究的方法。
在每一个小的分类中，其代表算法被列在圆形方框内。
算法适应性方法（Problem Transformation）是在现有的多标签学习方法的基础上对算法进行研究，其代表算法被列在圆形方框内。

如果从分类策略上对多标签学习方法进行研究，则根据标签之间相关性将算法策略分为如下罗列的三个类别：

1）一阶策略（First-Order Strategy）：一阶策略会考量标签之间的相互独立性，将多标签学习问题转换为普通的分类问题。

2）二阶策略（Second-Order Strategy）：二阶策略是考虑标签之间的两两相关性，构造多标签学习系统，
虽然因为在一定程度上考虑了标签之间的相关性，所以系统的泛化性能较优，但是也会导致计算复杂度显著提升。

3）高阶策略（High-Order Strategy）：相较于二阶策略，高阶策略的考察对象是高阶标签。  
一组随机标签集合的相关性是高阶策略的实施对象。构造多标签学习系统。
该策略可以真实反映问题的标签相关性，但是模型复杂度就会相较于二阶策略来讲更高。在处理大规模的学习问题方面比较吃力。

\subsection{skip-gram}
基于机器学习的方法，比如SVM，MaxEnt，朴素贝叶斯（Naive Bayes）等方法等将情感分析任务转换为分类问题~\upcite{DBLP:conf/emnlp/PangLV02}。
这些模型结合词袋的词表示形式和其他有用的特征来一起训练分类器。
相较于情感词典方法，这一类方法在训练过程中使用特定领域的数据较多。
这一类方法在许多应用中证明了其有效的分析情感的能力~\upcite{khan2015combining}。

但是，在实现过程中，词袋功能虽然便捷，但是不能有效传递单词的句法和语义关系~\upcite{DBLP:conf/semeval/HouTWZXC15}。
两个相似的词语之间可能有完全不同的表达方法~\upcite{DBLP:conf/semeval/HouTWZXC15}。
为了克服这一问题所带来的局限性，近些年来，研究者们通过从大量文献中学习联合表示法（jointly representation），
提出了低维词嵌入的概念~\upcite{DBLP:journals/jamia/NikfarjamSOGG15}。
\begin{figure}[h]
	\centering
	\includegraphics[width=280bp]{figures-atGRU/skipgram.pdf}
	\caption{Skip-gram模型详细架构}
	\label{fig:skip-gram}
\end{figure}

Skip-gram模型如图~\ref{fig:skip-gram}所示，是训练大量无标记语料库词嵌入的通用方法。
在该模型中，通过最大概率${p\left(o|c;\theta \right)}$来获得词嵌入。
\begin{equation}
\widehat{y}=p\left ( o|c \right )=\frac{exp\left ( u_{o}^{T} v_{c}\right )}{\sum_{w=1}^{W}exp\left ( u_{w}^{T} v_{c}\right )}
\end{equation}

其中，${v_{c}}$是在skip-gram里中心词${c}$的词嵌入，${w}$则表示第${w}$个单词。
对于所有的词汇，输出词嵌入由${u_{w}}$表示，${w=\left ( 1;...;W \right )}$。
单词${o}$是预期得到的词，${u_{o}}$则是它的词嵌入。
在这个等式中，损失函数由交叉熵~\upcite{DBLP:journals/access/HaberBQH17}计算得到：
\begin{equation}
J\left ( o,v_{c},U \right )=CE\left ( y,\widehat{y} \right )
\end{equation}

\section{基于深度学习的情感分析方法}
\subsection{机器学习概述}
%【不要！！】  
% 机器学习是计算机科学的一个领域，它使计算机能够在没有明确编程的情况下进行学习~\upcite{DBLP:journals/ibmrd/Samuel00}。
% 机器学习技术为现代社会的许多方面提供了强大的力量:从网络搜索到社交网络上的内容过滤，
% 再到电子商务网站的推荐，而且越来越多地出现在相机和智能手机等消费产品中。
% 机器学习系统用于识别图像中的对象，将语音转换为文本，将新闻项目，帖子或产品与用户的兴趣相匹配，并选择相关的搜索结果~\upcite{goodfellow2016deep}。
机器学习（Machine Learning）是计算机科学的一个领域，它使计算机能够进行训练并完成任务并且是在没有明确编程的情况下~\upcite{DBLP:journals/ibmrd/Samuel00}。
随着计算能力的逐步提升，现在机器学习已经在存在于人类生活中的方方面面。基础的应用包括识别
图像，自然语言转换，推荐系统等等。包括国防系统中的危险感知，都应用了机器学习技术。随着算法优化
以及手机CPU能力的提升，不少智能移动设备中，也部署了机器学习算法。

机器学习的核心目标就是通过机器学习到的历史经验进行概括预测~\upcite{DBLP:journals/ibmrd/Samuel00}。
泛化能力是机器学习领域的一个重要概念，它指学习机器在对一个数据集进行学习之后，对未来任务的准确执行的能力~\upcite{DBLP:journals/ftml/MuandetFSS17}。

为了在泛化的上下文中取得最好的性能，假设复杂性应该与数据背后的函数的复杂性相匹配。
如果假设没有函数复杂，那么模型就不适合所预想的训练数据。
如果模型的复杂性增加，那么训练误差就会随之减小。
但如果模型过于复杂，那么就会产生过拟合问题，泛化会变得越来越差~\upcite{DBLP:journals/ftml/MuandetFSS17}。

根据学习经验的原则，机器学习可以分为两种学习模式，监督学习和无监督学习。
监督学习中，每一个实例或样本都对应一个标签或目标，这些标注都需要人为给文本一个类标签。
而无监督学习每个实例或样本都是没有标签的，在观察到是随机向量的多个样本之后，试图隐式或显式地学习其概率分布。

\subsection{深度学习概述}
在二十世纪初，机器学习领域常常受到计算能力的限制，导致研究人员在设计模型时，往往有许多束缚，模型都
比较简单。随着近年GPU加速计算的盛行，模型开始变得更大，需要训练的参数更多，尤其是以神经网络
为代表的模型~\upcite{goodfellow2016deep}。这种深层次的神经网络结构以及与其类似的大型模型被称作深度学习（Deep Learning）。深度学习能够极大
的提高数据拟合的能力~\upcite{goodfellow2016deep}，可以提取多层级，多级别的特征。大型卷积神经网络可以很好的解决图像类问题。而深度RNN模型的提出也为自然语言方面
的研究提供了解决方案。

%【不要！！】 
% 传统的机器学习技术在以原始形式处理自然数据的能力方面受到限制。
% 数十年来，构建模式识别或机器学习都需要精心的工程设计和大量的领域专业知识来设计一个特征提取器。
% 这个特征提取器用于将原始数据，如图像的像素值，转换为适当的内部表示，或学习子系统，
% 从而可以检测或分类输入中的模式~\upcite{goodfellow2016deep}。
% 深度学习允许由多个处理层组成的计算模型来学习具有多个抽象级别的数据表示~\upcite{goodfellow2016deep}。
% 这些方法极大地改进了语音识别，视觉识别，目标检测和许多其他领域，比如医学药物学，基因学等领域的最新技术。
% 深度学习通过使用反向传播算法来发现大型数据集中的复杂结构，以指示机器应如何改变其内部参数。
% 深度卷积网络在处理图像、视频、语音 和音频方面取得了突破性进展，而循环神经网络给文本和语音等连续数据带来了研究突破的希望~\upcite{goodfellow2016deep}。

\subsection{基于循环神经网络的语言模型}
循环神经网络（RNNs）从属于ANNs，主要特点是表现出非静态的时间行为~\upcite{DBLP:journals/pami/GravesLFBBS09}，因为单元之间是有向循环的。
与前馈神经网络不同，循环神经网络可以使用其内部存储器来处理任意输入序列。
这使得它们适用于手写识别或语音识别等任务。
像所有的神经网络一样，RNN的单元为其多个输入分类一个权重矩阵，这些权重代表各个输入在网络层中所占的比重。然后，对这些权重应用一个函数来确定单个输出，这个函数一般被称作损失函数或代价函数，
用于限定实际输出与目标输出之间的误差。
RNN相较于传统的神经网络而言，最大的优势在于，在对当前输入分配权重的同时，也将过去时刻输入的权重纳入分配考虑之中。

Tomas等人在2010年提出了一种全新的基于循环神经网络的语言模型 (RNN Language Model)。
结果表明，与现有技术的退避语言模型相比，通过使用多个RNN的混合模型，可以减少大约50\%的复杂度\upcite{DBLP:conf/interspeech/MikolovKBCK10}。
本文提出的基于注意力机制的双向循环GRU框架的第一层次部分也是受到语言模型的启发，从“句子序列建模”和“词特征理解”两个角度来分析情感，对网络在线评论本文进行情感极性的分类。
“句子序列建模”是从语义依赖角度建立情感感知体系，而“词特征理解”则是从“词元级别”将句子细化，对其关键信息进行捕捉。

最原始的语言模型结构如图~\ref{fig:LMLM}所示。在图示的RNN语言模型中，本文以句子\emph{The students opened their books.}为例，语言模型先将前四个单词分别输入到RNN中。
目标是通过RNN来预测下一个单词。

\begin{figure}[h!]
	\centering
	\includegraphics[width=280bp]{figures-atGRU/LMLM.pdf}
	\caption{语言模型的详细结构}
	\label{fig:LMLM}
\end{figure}

首先，语言模型将句子中每个单词都转换成一个独热向量（one-hot vector），
句子中第${i}$个单词用${x^{<i>}}$来表示，其中，${x^{<i>} \in \mathbb{R}^{\left | V \right |}}$。
词嵌入${e^{<i>}}$的计算如下：
\begin{equation}
e^{i}=Ex^{<i>}
\end{equation}

之后，将词嵌入输入到RNN语言模型的隐藏层中，隐藏层状态如公式~\ref{eq:hidden}所示，
${h^{<0>}}$表示初始的隐藏状态。
句子中下一个单词根据前一个单词预测得到，由于RNN具有时序性的特性，
所以预测单词${x^{<t>}}$的结果由隐藏状态${h^{<t-1}}$共同决定。
\begin{equation}
h^{<i>}=\sigma \left ( W_{h}h^{i-1} +W_{e}e^{i}+b_{1}\right )
\label{eq:hidden}
\end{equation}

在最后一个时间步长中，隐藏单元输出最后要预测的单词\emph{books}，${b}$表示偏置单元，计算方式如下：
\begin{equation}
\widehat{y}^{<t>}=softmax\left (  Uh^{<t>}+b2\right ) \in \mathbb{R}^{\left | V \right |}
\end{equation}

基于RNN的语言模型在情感分析领域相较于传统神经网络模型具有很多优势，现详细罗列如下所示：

1）RNN模型可以处理任意长度的输入，并且模型的大小不随输入序列增长而增大。

2）在计算第${t}$个时间步长时可以利用很多步之前的信息。

3）通过时间步长，RNN的权重可以相互分享。

RNN语言模型也存在一些缺陷，比如，循环计算的过程比较缓慢。在实际操作中，获取很多步长之前的信息较为困难。
但是，它最主要的局限性在于梯度消失和爆炸问题。首先，本文需要了解为什么会产生梯度消失问题，它的根源是由于神经网络和反向传播。
由于RNN的发展，本文可以构建层次更为复杂更为深度的网络结构。然而，由于网络层数太多太深，
导致梯度无法继续传播，激活函数达到了饱和性。而梯度爆炸问题顾名思义是由于梯度太大，导致训练结果不会收敛。
研究者们提出了一些通过调整超参数的方法，但是随着神经网络的发展，还有一些更高级的网络结构在完美解决梯度消失和爆炸问题额同时，
保持RNN的主要优势。比如，LSTM和GRU，本文的框架也是基于GRU的改进基础上发展的。

\subsection{双向循环神经网络}
双向循环神经网络（Bidirectional Recurrent Neural Network）的提出目的在于希望可以增加RNN中输入信息的数量。传统的神经网络，例如多层感知器（MLPs）和时延神经网络（TDNNs）
对输入数据的灵活性非常有限制，因为它们都需要固定输入数据。传统的循环神经网络，例如LSTM和GRU，
也因为未来时间步长的输入信息无法从当前的状态到达。
但是，这些缺陷都可以在双向循环神经网络中解决，原因主要有两点。
第一，双向循环神经网络不需要输入数据是固定长度的，它可以从输入序列的任意一端开始处理序列。
第二，双向循环神经网络也因为其网络结构的特殊性，可以从当前状态访问未来的的输入序列信息。
双向循环神经网络的基本思想是将输入序列的两个不同方向的隐藏状态连接到相同的输出，
它不仅可以获得之前单词的信息，也可以获得之后单词的信息，并且将两者进行结合考虑分析情感。
如图~\ref{fig:brnn}所示，本文绘制出了浅层的双向循环神经网络的详细结构，
可以看出，每一个记忆单元将中间顺序的表现形式传递给下一个。
关于双向循环神经网络更进一步的研究，将会在本文第三章中阐述。

\begin{figure}[h!]
	\centering
	\includegraphics[width=250bp]{figures-atGRU/BRNN.pdf}
	\caption{浅层双向循环神经网络的详细结构}
	\label{fig:brnn}
\end{figure}

\subsection{门控制循环单元}
针对于循环神经网络所存在的梯度消失和爆炸问题，研究者们设计了很多高级的网络体系结构，比如LSTM和GRU。
除此之外，还有一些学者使用正则化的表达式来强制错误信号不逐渐消失~\upcite{DBLP:conf/icml/PascanuMB13}。
传统的RNN网络可以沿着输入序列开始至结束单向处理输入序列。

门控制循环单元（Gated Recurrent Unit）的内部结构如图~\ref{fig:GRUunit}所示，其网络结构的优势主要有以下几点：

1）GRU网络将LSTM网络中的遗忘门（forget gate）和输入门（input gate）相结合成一个新的门控制单元：更新门（update gate）。
更新门可以更好地控制过去和当前输入序列之间的信息流的大小~\upcite{DBLP:journals/corr/ZhangXS17}。
GRU比LSTM少了一个门控制单元，所以它也是LSTM的简化版，可以更高效地进行运算。

2）不仅如此，GRU也是香草RNN（vanilla RNN）的扩展版本，它能够缓解RNN网络的梯度消失和爆炸问题~\upcite{DBLP:conf/eacl/MousaS17}。

3）GRU网络结构可以捕捉远距离的依赖关系。

4）GRU网络结构允许错误的信息根据输入向不同权重分布流动。

GRU的内部结构如图~\ref{fig:GRUunit}所示。
\begin{figure}[h!]
	\centering
	\includegraphics[width=250bp]{figures-atGRU/GRUunit.pdf}
	\caption{门控制循环单元的内部结构}
	\label{fig:GRUunit}
\end{figure}

因此，在本文的框架中，首先设计了一个预注意力双向循环GRU网络层来学习从“语句”级别学习句子序列级的单词和语法依赖性。
与单向的网络层次结构相比，本文的框架可以最大程度地利用上下文中的交互式信息。

\section{注意力机制}
注意力机制首先应用于图像分类中~\upcite{DBLP:conf/nips/MnihHGK14}，Bahadanau和Cho等研究者们又对其进行了改进，
他们将注意力机制首先应用于了神经机器翻译（Neural Machine Translation: NMT）中。
NMT里面的注意力机制可以让解码器关注句源或词源的特定部分，这也就是说，所有隐藏层状态的相关信息都可以被注意力机制考虑进去~\upcite{DBLP:journals/corr/BahdanauCB14}，
而非像传统模型一样，只考虑最后一个隐藏状态的信息。不仅如此，注意力机制还提供了对远距离状态的快捷方式，这有助于避免梯度消失问题。

注意力机制不仅应用于神经机器翻译中，还被很好地应用于文本阅读理解中。它有效解决了瓶颈问题（bottleneck issue），
帮助提高了对评论的情感理解。注意力机制的提出主要有以下明显的优势：

1）注意力机制减小了运算复杂度和负担，降低了数据的维度。

2）注意力机制最大的贡献在于可以提高情感分析的准确率，因为它可以更专注地找到输入文本中最明显与情感相关的特定有用信息。

3）如果解决较长的文本语句时，上下文的特征往往会在较长序列中损失部分信息，但是注意力机制对所有隐藏层的状态进行了分权重的连接关注，
所以可以很好地解决信息丢失的问题。

下面本文将详细介绍目前被广泛使用的一些注意力模型。

\subsection{单层注意力模型}
单层注意力模型是在2014年由Bengio等人提出的~\upcite{DBLP:conf/nips/MnihHGK14}，他们利用单层的注意力模型有效地解决神经机器翻译中
源文本中语句的对齐问题。
在机器翻译中，解码器-译码器（encoder-decoder）模型是最常用的模型。
假设译码器已经获得原文本的语句，用${x=\left ( x_{1},x_{2},...x_{t} \right )}$表示，然后在句子中每一个单词都转换成词向量之后，
一个句子可以表示为${\left ( h_{1},h_{2},...h_{t} \right )}$。${i}$表示需要生成的目标翻译句子的第${i}$个单词。
单层注意力模型的基本思想在于，对于生成的目标语句中的特定单词，只会和源文本语句中的特定单词相关。
该相关性可以由概率计算得到，即是生成目标翻译语句中的单词${y_{i}}$的概率。
计算过程是需要首先对当前上下文语境和每个源文本语句中单词的相关性进行计算，然后利用softmax分类器将相关程度转换成概率模型：
\begin{equation}
p\left ( y_{i}|y_{<i},x \right )=softmax\left ( W_{s}\widetilde{h}_{i} \right )
\end{equation}
单层的注意力模型中隐藏状态${\widetilde{h}_{i}}$则可以表示为：
\begin{equation}
\widetilde{h}_{i}=tanh\left ( W_{c}\left [ c_{i};z_{i} \right ] \right )
\end{equation}
其中，${c_{i}}$是当前目标翻译端${y_{i}}$在句子中的上下文，${z_{i}}$是目标翻译端的隐藏层状态，由公式~\ref{eq:zi}计算得到：
\begin{equation}
z_{i}=f\left ( z_{i-1},y_{i-1},\widetilde{h}_{i-1} \right )
\label{eq:zi}
\end{equation}

接下来，需要获得对生成翻译，也就是预测目标的注意力贡献，它由相关性概率乘以源文本语句中的隐含层状态计算得到，
再将源文本中每个单词对模型的贡献求和，作为预测下一个翻译生成词汇的输入的一部分。

\subsection{基于位置的注意力模型}

基于位置的注意力模型（Potential Attention Model）是在2017年由Qin等人~\upcite{DBLP:conf/sigir/ChenHHHA17}提出的。
研究者们将传统的注意力机制进行变形之后用于问答系统（Question Answer）之中。
首先，需要得到问答系统中问题句子的全部单词，转换成词向量之后，经过基于位置的注意力模型计算，输出答案句子中所有的单词词向量。

基于位置的注意力模型的原理是，对于问题语句中某一个单词，如果它也出现在了回答语句中，那么，回答语句中这个单词上下文语境之内的词语影响权重更大。
这个影响权重呈高斯分布，随着距离的改变而改变。如下面的公式所示：
\begin{equation}
\alpha _{j}=\frac{exp\left ( e\left ( h_{j},p_{j} \right ) \right )}{\sum_{k=1}^{l}exp\left ( e\left ( h_{k},p_{k} \right ) \right )}
\end{equation}
通过拟合高斯分布，可以得到所有输入序列中单词的影响概率。这个影响概率就是上文所提到的影响权重，
随后，将影响权重和输入语句中的所有单词词向量加权求和，得到基于位置的影响向量：
\begin{equation}
e\left ( h_{j},p_{j} \right )=v^{T}tanh\left ( W_{H}h_{j}+W_{p}p_{j}+b \right )
\end{equation}
基于位置的影响向量就是Potential Attention Model的输出，如公式~\ref{eq:ra}所示，它可以指定输入序列中所有单词的隐藏词向量和影响向量的相关程度。
\begin{equation}
r_{\alpha }=\sum_{j=1}^{l}\alpha _{j}h_{j}
\label{eq:ra}
\end{equation}

\subsection{多级注意力机制}

除了上面介绍的最近一年新出现的几种注意力机制模型，还有一种多级注意力模型（Hierarchical Attention）~\upcite{DBLP:conf/kdd/LiMGK17}。
该模型用于问答系统中。
对于输入序列，是源文本中所有语句的所有词向量和给定的问题语句中的隐藏向量。
通过多级注意力机制，最后输出得到答案语句中的所有单词。

多级注意力机制对于源文本中的每一个语句，首先计算第${i}$个单词和所问问题的相关程度，
然后，多级注意力模型的第一层用来计算得到隐藏单元的向量。多级注意力模型的第二层用于得到上下文向量，最后生成预测的回答。
多级注意力机制的详细架构如图~\ref{fig:caan}所示。

\begin{figure}[h!]
	\centering
	\includegraphics[width=300bp]{figures-atGRU/caan.pdf}
	\caption{多级注意力模型的详细架构}
	\label{fig:caan}
\end{figure}

多级注意力机制除了可以用在问答系统中，也有学者将其用在对文章的摘要提取~\upcite{DBLP:conf/sigir/RenCRWMR17}中。
对文章的摘要提取任务，是将一个文档中所有的词向量作为输入，输出是0或1，表示每一个句子能否作为摘要，
相当于是一个二分类的预测。
摘要提取的一个关键是判断哪些词语或者句子更具有代表性，更能代表其他词语或者句子。
通常的做法是对于每一个句子${t}$，计算${\left [ 1,t-1 \right ]}$，${\left [ t+1, \right ]}$与${t}$的余弦相似度，
来作为判断句子${t}$能否作为摘要的依据。

首先，对每一个单词计算其对句子的贡献率，即影响概率。通过单词级别（word-level）的注意力模型计算生成的向量作为句子的隐含向量；
而在句子级别（sentence-level）计算每个句子和其前面${t-1}$个句子的相似度，利用第二层注意力生成的上文相关向量，
同时利用第二层注意力生成的下文相关向量，计算每个句子和其后面所有句子的相似度。
最后将上文相关向量和下文相关向量作为最终预测的一部分输入。

上述两种多级注意力机制的应用都是用了单词级别和句子级别的多级注意力机制，本文提出的框架也受到了分别从单词和句子两个层级来进行自然语言处理的启发。
多级注意力机制的应用也十分广泛，除了在自然语言处理领域，在其他领域也有应用。
比如Tong等人~\upcite{DBLP:conf/kdd/TongKIYKSV17}介绍了如何利用多级注意力模型帮助生成岩石描述。
该模型的输入是钻井得到的多类数据和词典中所有单词的词向量，每种数据的数值随时间或者其他维度不断变化。
输出则是岩石描述的文本。
该论文提出的第一层注意力模型是为了判断，在预测下一个描述词时，每类数据中具体哪个数值与该描述词有关。
第二层的注意力模型的意义在于判断预测下一个描述词时，哪一类数据与该描述词有关。

\section{本章小结}
本章主要介绍了情感分析领域的相关工作，首先，本文介绍了情感分析的计算语言学方法，作为较为传统的方法，
情感语言学方法有一定的局限性，只能给出单词在一般语言环境下的情感打分，但是不同单词在不同领域范围内的情感极性是不一样的。
所以，接下来，本文介绍了基于机器学习的情感分析方法，主要有单标签学习方法，多标签学习方法和skip-gram。
随着近几年深度学习的循序发展，基于深度学习的情感分析方法逐渐称为了学术界的主流。
本文主要介绍了人工神经网络，尤其是循环神经网络在此领域的应用。
RNN具有学习单词和单词之间基本关系的优势，然而，随之带来的是梯度消失和爆炸问题，所以LSTM和GRU逐渐开始被使用。
然后，本文分析了GRU的优势，并讨论了句子级别学习和语法依赖的重要性。
我们还突出强调了从单词特征层面关注输入序列的关键信息的重要性，介绍了近几年被广泛应用的注意力机制，包括单层注意力机制，基于位置的注意力机制和多级注意力机制。
这些基本的相关工作都为下一章本文提出的框架进行了铺垫。









