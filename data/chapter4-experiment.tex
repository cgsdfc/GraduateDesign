% !Mode:: "TeX:UTF-8"
% Author: Zhengxi Tian
% Email: zhengxi.tian@hotmail.com

\chapter{实验结果与讨论}\label{ch:experiment}
本章从系统层面得分，句子层面得分和定性分析三个方面展示了实验的数据与结论。
在众多数据中，我们发现以下主要的结论：
\begin{enumerate}
    \item 同一个数据集上的模型的得分差异较小，不同数据集上的模型得分差异较大。
    \item 各个指标的分布情况呈现集群现象，同一集群内的指标分布相似，不同集群之间的指标分布迥异。
    \item 尽管在某些指标或数据集上一个模型一致的超过另一个模型，但是在全部指标和数据集上，没有这种现象。
\end{enumerate}
我们把上述结论归因于以下几个方面：
\begin{enumerate}
    \item 生成式对话的合理响应空间巨大，响应具有很高的熵，给评价增加了难度。
    \item 模型在不同数据集上的泛化能力有待加强。
    \item 对话数据集的质量参差不齐，而模型的质量和数据集的质量紧密相关。
    \item 指标捕捉了错误的表征，给评价模型的性能加入了噪音。
\end{enumerate}
以下所有图片均使用seaborn绘制\footnote{\url{http://seaborn.pydata.org/}}。

% ------------------------- %
% ----- System Level ------ %
% ------------------------- %
\section{系统层面得分}\label{sec:system_scores}
表~\ref{tab:systemScoresAll}~展示了各个模型在不同数据集上测定的各项指标的系统层面得分，
加粗的得分是三个系统中的最优者，
句子平均长度（\#words）不是作为一个指标而是作为一个参考数据出现的，
所以没有加粗。
从表中的数据来看，
在LSDSCC上HRED取得了除ROUGE-4外所有指标的最优，
在OpenSubtitles上，VHRED取得了除了ADEM和Distinct-N之外所有指标
的最优，
在Ubuntu上情况比较复杂：HRED取得9个指标的最优，
LSTM取得了6个指标的最优，
VHRED取得了3个指标的最优。
如果我们把在一个数据集上取得最优指标最多的模型称为在该数据集上的
最优模型的话，
从总体上看，HRED是所有数据集上的最优模型，
因为它在LSDSCC和OpenSubtitles都是最优模型。
但是观察在每一项指标上取得最优的模型时我们发现，
不同数据集上的取得最优的模型往往不相同。
\input{data/system_scores.tex}
为了了解不同模型在不同数据集上的表现，
我们对每一个指标的系统层面得分绘制了柱形图。

\input{data/barplot_group/BLEU.tex}
图~\ref{fig:BLEU_system}~是BLEU的系统得分柱形图。
BLEU-1和BLEU-2的情况比较类似：
从总体上看，模型在Ubuntu的得分普遍高于在另外两个数据集上的得分，
而HRED和VHRED在所有数据集上都比LSTM表现好，
在Ubuntu上，VHRED超过了HRED，但在另外两个数据集上，
HRED都一致的超过了VHRED。
模型的BLEU-1得分在虽然在不同数据集之间相差较大，
但在同一个数据集内却相差不大。
然而，模型的BLEU-2，BLEU-3，BLEU-4得分即便在同一个数据集内也相差很大。

BLEU-3和BLEU-4的情况比较类似：
从总体上看，没有哪一个数据集的得分要普遍高于其他数据集上的得分。
在所有数据集上，HRED和VHRED仍然普遍比LSTM得分高，
虽然有时候优势不太明显 。
VHRED在OpenSubtitles上大幅度的超过了HRED，
但在另外两个数据集上，HRED都超过了VHRED。
从绝对数量的角度来看，BLEU的得分随着N的增大而减小，
表现在图上就是，$N = i$的BLEU最低分不低于$N = i + 1$的最高分，
这可能是因为两个句子的n-gram共现数随着$N$的增大而减少。

\input{data/barplot_group/ADEM_EB.tex}
图~\ref{fig:ADEM_EB_system}~是基于词嵌入的指标和ADEM的系统得分柱形图。
有趣的是，ADEM对所有数据集上的所有模型的打分都非常接近2.6。
从表~\ref{tab:systemScoresAll}~看出，
当数据集不同时，ADEM的分数在百分位变化，
当数据集相同而模型不同时，ADEM的分数在千分位或之后变化，
这表明数据集对ADEM得分的影响要大于模型的影响。
\begin{table}[H]
    \centering
    \caption{ADEM的系统层面打分}
    \label{tab:ADEM_system}
    \begin{tabular}{llll}
        \toprule
        & HRED & LSTM & VHRED \\
        \midrule
        LSDSCC & 2.6178 & 2.6127 & 2.6163  \\
        OpenSubtitles & 2.6228 & 2.6224 & 2.6219 \\
        Ubuntu & 2.6353 & 2.6381 & 2.635 \\
        \bottomrule
    \end{tabular}
\end{table}
由于从图~\ref{subfig:ADEM_system}~中难以得分的优劣，
我们将从表~\ref{tab:ADEM_system}~得出结论：
首先，从数据集的角度，所有模型在Ubuntu上的得分好于在OpenSubtitles的得分，
而后者要好于在LSDSCC上的得分。
其次，从模型的角度，在LSDSCC上，HRED的得分好于VHRED的得分，
后者要好于LSTM的得分。
在OpenSubtitles上，HRED的得分好于LSTM的得分，
后者要好于VHRED的得分。
在Ubuntu上，最好的模型是LSTM，其次是HRED，最后是VHRED。
从上述分析来看，HRED在两个数据集上得分最优，
VHRED在两个数据集上得分最差，
LSTM在三个数据集上排名各不相同。
尽管得分的差异很小（最大值和最小值之间只相差0.019），
而且各个数据集上模型的排名都不相同，但是还是可以看出，
HRED在ADEM指标上表现较好，而VHRED则较差。
这可能是因为我们没有用预训练的HRED初始化VHRED。

和ADEM相比，基于词嵌入的指标表现的更具有一致性。
Average，Greedy和Extrema三种指标虽然在绝对数值上有些差异，
但是图像的模式却非常相似。
从总体来看，在所有数据集和指标上，HRED和VHRED都比LSTM表现的好。
我们发现在LSDSCC上，这三个指标很难将模型区分开来，
事实上，所有模型在LSDSCC上的某个指标都倾向于得到相同的分数。
我们猜测这是因为LSDSCC的测试集样本太少（300个），
没有足够多的样本令模型之间的差异展现出来。
在测试集样本数比较多的OpenSubtitles上，三个模型的区别比在LSDSCC上明显，
但是由于OpenSubtitles的噪音比较大，话题比较分散，
模型相对比较容易产生话题相关的响应，
导致三个模型的区分不是特别明显。
在Ubuntu上，三个模型具有最明显的区分，
HRED和VHRED超过了LSTM并拉开了较大距离。
我们猜测这是因为Ubuntu是一个技术领域的数据集，
含有大量技术词汇，如\texttt{apt-get}，\texttt{java}等等。
模型必须能捕捉到消息中的技术相关的语义并生成相关的句子才能的高分，
这要求模型对消息的主题有很强的捕捉能力。
HRED和VHRED比LSTM多了编码器结构，可以肯定它们捕捉消息主题的能力更强。
此外，模型在三个数据集上的区分度的不同也和数据集的对话轮数有关，
LSDSCC是单轮对话，OpenSubtitles是3轮对话，而Ubuntu是多轮对话。
多轮对话数据集更有利于能够利用它们的HRED和VHRED。

\input{data/barplot_group/ROUGE.tex}
图~\ref{fig:ROUGE_system}~是ROUGE的系统得分柱形图。
从总体上看，
不同数据集上的模型在ROUGE-1，ROUGE-2，ROUGE-L和ROUGE-W的得分都不为0，
但在ROUGE-3和ROUGE-4上，多个模型的得分接近0，
这是正常的，因为一般来说，响应和参考之间的高阶n-gram重叠非常少。
ROUGE-1，ROUGE-L和ROUGE-W的图像非常相似。
在这三个指标上，在LSDSCC和OpenSubtitles上，HRED和VHRED超过了LSTM，
但在Ubuntu上，LSTM却超过了HRED和VHRED；
除此之外，各个数据集内三个模型的排名也基本一致，
但是模型之间的差距有时很大，比如LSDSCC上的ROUGE-2，
有时很小，比如OpenSubtitles上的ROUGE-1。

在ROUGE-3和ROUGE-4上，LSTM的得分在所有数据集上普遍很低，
而HRED和VHRED在某些数据集上得分也很低，
在Ubuntu上，所有模型的得分都很低。
令人注目的是，VHRED在OpenSubtitles和LSDSCC上都取得了不错的成绩，
而HRED的表现在LSDSCC上较好，在OpenSubtitles上较差。

\input{data/barplot_group/Other.tex}
图~\ref{fig:Other_system}~是Distinct-N，
METEOR和句子长度的系统得分图。
先看句子长度，在Ubuntu上的模型的句子长度明显大于
在LSDSCC上的模型，而后者大于在OpenSubtitles上的模型。
从第~\ref{sec:dataset_proprecessing}~节我们知道，
在训练集的平均句子长度上，有Ubuntu > LSDSCC > OpenSubtitles。
模型生成的响应的长度和训练集的平均句子长度具有一致性，
这是因为训练的目标函数是最大化训练集样本的对数概率，
模型倾向于模仿训练集的统计特征。

Distinct-N对模型的区分度并不高，
在同一个数据集上训练的模型，它们的Distinct-N都比较接近。
从数据集的角度，OpenSubtitles上的模型的Distinct-1较高，
而Ubuntu上的模型的Distinct-2较高。
METEOR似乎受句子长度的影响较大，
在句子长度较大的Ubuntu上，模型的METEOR得分要比其他数据集上的得分高得多。
而LSDSCC上的METEOR也比OpenSubtitles上的得分略高。
METEOR对不同模型的区分度不是很大，
从总体上看，HRED和VHRED比LSTM的表现要好。

本节利用柱状图分析了模型，数据集和指标的系统层面得分。
在一些图中，各个模型或者数据集的得分差异不十分明显，
为了突显得分的差异，我们把观察的维度分成数据集和模型，并分别绘制了箱体图。
我们得出的结论是相同的，但是由于这些箱体图数量较多，所以放在了附录。
附录~\ref{ch:dataset_system_dist}~和
附录~\ref{ch:model_system_dist}~分别从
数据集的维度 和 模型的维度 展示了不同指标的系统层面得分。

% --------------------- %
% -- Utterance Level -- %
% --------------------- %
\section{句子层面得分}\label{sec:utterance_scores}
我们还从句子层面得分方面进行了分析。
因为模型生成的句子$u$可以看做一个句子空间$U$上的随机变量，而指标是一个确定的函数$f_{s}$，
所以可以把句子层面的得分看做一个随机变量$\lambda_u = f_{s}(u)$。
于是我们便可以用描述统计学（Descriptive Statistics）和统计推断断（Statistical Inference）
的方法来分析指标在句子水平的情况，包括单变量和双变量的方法，以及各种统计数据。

我们绘制了各种指标的句子层面得分的单变量分布（Univariate Distribution）。
为了便于从图像上比较各种指标的分布特征，我们对数值作了归一化（Scaling）处理，使数据的平均值为0，标准差为1：
\begin{align}
    x' = \frac{x - \mu}{\sigma}
\end{align}
我们绘制了所有模型和数据集的组合上的分布图。
图片的数量非常多，为了使读者能快速检阅所有指标的分布情况，
我们在正文只展示了模型为HRED，数据集为OpenSubtitles
的各项指标的分布。
请参阅附录~\ref{ch:metric_dist}~查看所有的组合上的分布。

\input{data/distplot_group/BLEU.tex}
图~\ref{fig:BLEU_dist}~是BLEU的N取1到4时的分布图。
由于加入了平滑处理，所以大部分得分都有非零数值，
然而正如\cite{HowNot}所说，大部分句子的得分非常低，
在图像上的反映是，在均值附近集中了大量的概率质量（Probability Mass）。
得分较高和较低的句子非常少，几乎没有。
比其更高的N值，Unigram匹配是最容易得到的，所以BLEU-1在
均值的右边有一个比较低矮的分布，并且峰值对应的横坐标偏离了均值0，
这说明有相当一部分句子的得分高于均值。
而低于均值的部分出现了一个高峰，说明也有大量句子得分低于均值，
但总体来说，得分高于均值的句子更多。
BLEU-2至4的分布都非常相似，峰值的横坐标比BLEU-1更接近均值。
从总体上来说，BLEU的分布形状十分尖锐，大量句子集中在均值或略高于均值附近，
图像不对称， 峰值比高斯分布的峰值$1 / \sqrt{2 \pi} \approx 0.4$高。

\input{data/distplot_group/ADEM_EB.tex}
图~\ref{fig:ADEM_EB_dist}~是基于词嵌入的指标和ADEM指标的分布图。
因为ADEM可以看做是一种更高级的词嵌入指标，所以把它和其他词嵌入指标并列。
四个图像均呈钟型曲线，峰值非常接近0.4，峰值的横坐标则非常接近0。
ADEM曲线显示出几乎完美的对称，Greedy曲线的对称度次之，
Average曲线向右边偏移，Extrema曲线向左边偏移。
ADEM和词嵌入指标有相似之处。
从句子向量的组合方式来看，
Average的组合方法是平均值，Extrema的组合方法是极端值，
而ADEM的则使用了预训练的VHRED的编码器。
与其他词嵌入指标不同的是，ADEM考虑了上下文的影响，
用神经网络对上下文，响应和参考三者加权，
并且显式的优化了和人类评价的相关性，
可见ADEM采用的机制更为复杂。
Lowe在\cite{ADEM}中指出，ADEM倾向于保守打分，
即人类评价给高分的句子，ADEM倾向于给稍低的分数。
从ADEM的曲线可以从某种意义上验证这一点，
得分高于平均值的句子与得分低于平均值的句子几乎一样多，
说明它不倾向于打高分或者低分。

\input{data/distplot_group/ROUGE.tex}
图~\ref{fig:ROUGE_dist}~是ROUGE系列指标的分布图。
从曲线的形状来看，大致可分为两类：
ROUGE-1，ROUGE-L，ROUGE-W是一类，
ROUGE-2，ROUGE-3，ROUGE-4是一类。
ROUGE-2这类图像的特点是几乎所有句子的得分都集中在均值，
除此之外没有其他得分。
因为我们没有对ROUGE指标做任何平滑处理，
我们猜想原因是除了极个别句子外，其他句子都得了零分。
这表明ROUGE-N指标当$N > 1$时捕捉不到任何n-gram重叠。
与BLEU不同的是，ROUGE-N不考虑$n < N$的其他n-gram，
导致它更容易给出0分。

另一方面，ROUGE-1类指标更像是BLEU-1指标的噪化版本，
它们都是双峰曲线，而且第一个峰值对应的点$(x_1, y_1)$和
第二个峰值对应的点$(x_2, y_2)$都有相似的坐标。
读者可以验证以下取值范围对BLEU-1，ROUGE-1，ROUGE-L，ROUGE-W都成立：
\begin{align}
    -1 < x_1 < 0, 1 < y_1 < 1.5 \\
    0 < x_2 < 2, 0 < y_2 < 0.5
\end{align}

造成上述现象的原因是，所有的响应和参考基本上没有$N > 1$的n-gram重叠，
因此基于最长公共子序列和它的加权版本的匹配都退化成了Unigram匹配，
相应的，ROUGE-1，ROUGE-L，ROUGE-W都退化成了BLEU-1。

\input{data/distplot_group/Other.tex}
图~\ref{fig:Other_dist}~是Distinct-N，METEOR和\#words的分布图。
我们发现Distinct-1的分布呈现两极分化，
在均值附近和均值右边较远处都聚集了大量句子。
Distinct-N的分母是句子的长度（\#words），
分子是句子中各异的n-gram数量。
从\ref{fig:words_dist}来看，句子的长度集中在均值附近，
长于均值的句子比短于均值的句子多，
这表明Distinct-N的值主要受分子影响。
Distinct-1的曲线表明，极大部分句子的各异的单词数量都接近平均值，
但也有少部分句子各异的单词数量远远小于平均值。
% -- Distinct-2
Distinct-2的图像没有出现两极分化，
可能是因为Bigram的空间比Unigram更大，
一个句子更容易出现各异的Bigram。
图像的特点是：
\begin{enumerate}
    \item 大部分句子集中在高于均值的一片区域。
    \item 个别低于均值的区间聚集了大量句子。
\end{enumerate}
这可以理解为模型在应对不同的消息时，
生成的句子质量不一，对一部分消息产生了平庸的响应，
对另一部分消息却产生了单调的响应。

尽管通常被分类成基于词重叠的指标，
METEOR的图像表现出了和其他基于词重叠的指标（BLEU，ROUGE）相当不同的性质。
它似乎能区分不同响应的质量，并且和句子长度有着某种联系。
图像在均值的右侧表现出指数衰减， 在左侧则是一个尖锐的单峰。
本质上，METEOR是基于Unigram匹配的，
但是我们尚不清楚METEOR的多种匹配模块，对齐算法和惩罚系数是如何
使它的分布变得与BLEU-1等指标的分布如此不同。

% -- Conclusion -- %
中心极限定理（Central Limit Theorems）表明：大量独立随机变量的叠加近似于高斯分布。
我们假设当句子足够多时，句子层面的人类评价近似于高斯分布。
据此我们认为，句子层面得分的分布接近高斯分布是一个指标和人类评价具有较高相关性的前提条件之一。
从上述各个指标的分布来看， ADEM和词嵌入指标具有很大的潜力。

最后要提醒读者，
本节的分析结果是从OpenSubtitles和HRED的组合中得出的，
并不能完全代表所有组合下的指标分布情况。
观察附录~\ref{ch:metric_dist}可发现， 一般来说，
指标的分布当模型不同而数据集相同时比较相似，
当模型相同而数据集不同时则有较大差异，
因此句子层面的指标分布受数据集的影响较大。

\section{定性分析}\label{sec:qualitative_analysis}

\section{结果与讨论}\label{sec:result_and_discussion}

\section{本章小结}\label{sec:experiment_conclusion}
