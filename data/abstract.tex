% !Mode:: "TeX:UTF-8"
% Author: Zhengxi Tian
% Email: zhengxi.tian@hotmail.com

% 中英文摘要
\begin{cabstract}
    尽管基于序列到序列的生成式对话系统已经能够生成自然而流畅的对话，
    然而这类模型普遍存在着生成单一枯燥的对话的倾向。对话系统的目标
    是生成多样的，有意义的，能引起人们兴趣的对话，为了实现这一目标，
    许多学者提出各种模型，从不同角度解决枯燥对话的问题。但是，由于
    面向闲聊的对话系统缺乏好的自动化评价指标，这个领域高度依赖于
    人类评估，从而使评估系统的代价高昂，规模难以扩大，主观性
    强。为了了解不同的评价指标的优缺点，本文在多个公开的数据集上
    训练多个生成式对话的模型，测定不同指标的句子级别分数，并进行指标
    之间的相关性分析。从实验中我们发现，不同的模型、指标和数据集所得的
    分数存在聚类现象。例如，有些指标的得分在很多数据集和模型的组合中
    都具有高度相关性。另一方面，当一个模型在不同的数据集上训练，并
    用某一指标测量时，得分也可能具有高度相关性。本质上，本文通过相关性
    来衡量两个概率分布在其潜在参数变化时是否具有一致性。
    通过实证研究，本文指出：指标并不总是能一致的衡量在不同的数据集上
    训练的不同的模型。同样的，在不同数据集上训练的模型也不是在所有指标上
    的表现都一致的好。本文提出的分析方法为学者们提供了研究现有的指标或者
    模型的优缺点的新视角，从而有助于他们提出改进方案。
\end{cabstract}

\begin{eabstract}
    Although the Seq2Seq-based generative dialogue systems     are able to generate natural and fluent responses, they have been long known for the inclination to generate simple and repeated responses. Towards the goal of generating diverse, meaningful and engaging dialogues, many researchers proposed various methods to address the problem of low-quality responses. However, it has been known that chat-oriented dialogue systems lack good automatic evaluation metrics and the field relies heavily on human evaluation, which is expensive, unscalable and subjective. To better understand the pros and cons of various automatic metrics, we trained a number of open-domain generative models on
    a few public-available datasets and measured their performances with various metrics. We then analyzed the utterance-level inter-metric correlation on the scores of the trained models. From the experiments we found that scores of various combinations of model, dataset and metric tended to cluster along some axises. For example, for most of the combinations of model and dataset, some metrics tended to highly correlate with one another, which form a cluster. On the other hand, when trained on different datasets, a model's performance measured by a certain metric may be highly correlates with one another. Essentially we use correlation to examine whether two distribution behave consistently when their latent parameters vary. Although desirable, we empirically conclude that the metrics did not measure all the models trained on different datasets consistently. Similarly, the models did not perform consistently when trained on different datasets or measured by different metrics. Our novel approach provides an new perspective from which researchers can learn the pros and cons of existing metrics and models and thus propose enhancements.
\end{eabstract}
