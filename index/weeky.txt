1.修复Jiwei的代码库，可展开2周
2.修复Serban的代码库。
3.收集数据集，下载速度很慢，wget和curl均无法实现断点下载。实现了get_chunk
4.安装docker和nvidia-docker
5.修复Adem指标模型，AutoTuring
6.修复Rubber指标模型，ref和unref
7.抢占式训练和训练进度监控方法。
9.从Seq2SeqChatbots中获得大量模型的情报。
10.收集METEOR指标时发现了大量的MT指标，均找到代码实现。
11.发现了Cornell数据集，元信息丰富，自带TurnTaking，规模中等，格式json，包括parliament，supreme，movie，reddit-small，tennis。reddit本身是多个subreddit组成的巨大的数据集。
12.在Opensub上训练Jiwei的模型，训练了一周还是只有无意义的输出。老师叫我不要顾着复现模型。
13.在Ubuntu上训练Serban的模型，一周，三个模型的PPL收敛到30,40和50，输出比较有意义。
14.在OpenSubTitles上训练Serban的模型，使用dialogue_3_6，

本课题力求对现有的面向生成的对话系统的评价指标做一个尽可能完备的文献综述。在对话系统领域,由于人类对话的多样性和歧义性,评价系统输出的响应是一个比较困难的问题,也是一个开放的学术问题。我们的工作是把目前所有的评价指标都整理出来,对它们作逐一的考察,在考察现在的评价指标的基础上,我们将分析不同评价指标在不同的数据集上的特点,总结出好的指标应该具有的优点,促进对话系统评估的自动化。

尽管基于序列到序列的生成式对话系统已经能够生成自然而流畅的响应,这类模型普遍存在着生成单调响应(GenericResponse)的倾向。对话系统的目标是生成多样的,有意义的,能引起人们兴趣的对话。为了实现这一目标,学者们提出了各种模型,从不同角度解决单调响应的问题。但是,由于这类系统缺乏好的自动化评价指标,这个领域高度依赖于人类评价,从而使评估系统的代价高昂,规模难以扩大,而且主观性强。为了了解不同的评价指标的优缺点,本文在多个公开数据集上训练了多个生成式对话模型,测定不同指标的句子级别分数,并进行指标之间的相关性分析。从中我们发现,不同的模型、指标和数据集所得的分数存在聚类现象。例如,有些指标的得分在很多数据集和模型的组合中都具有高度相关性。另一方面,当一个模型在不同的数据集上训练,并用某一指标测量时,得分也可能具有高度相关性。通过实验,本文指出:指标并不总是能一致的衡量在不同的数据集上训练的不同的模型,而在不同数据集上训练的模型也不是在所有指标上的表现都一致的好。本文提出的分析方法为学者们提供了研究现有的指标优缺点的新视角,从而有助于他们提出改进方案。