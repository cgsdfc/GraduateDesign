% !Mode:: "TeX:UTF-8"
% Author: Zhengxi Tian
% Email: zhengxi.tian@hotmail.com

\chapter{实验结果与讨论}\label{ch:experiment}
本章从系统层面得分，句子层面得分和定性分析三个方面展示了实验的数据与结论。

\section{系统层面得分}\label{sec:system_scores}
图~\ref{tab:systemScoresAll}~展示了各个模型在不同数据集上测定的各种指标的系统层面得分，
加粗的得分是三个系统中的最优者。
从表中的数据来看，在LSDSCC上的最优模型是HRED，
在OpenSubtitles上的最优模型是VHRED，
在Ubuntu上的最优模型是HRED，它在9个指标中取得了最优，但是LSTM也在6个指标中取得了最优。
\input{data/system_scores.tex}

为了了解不同模型在不同数据集上的表现，我们对每一个指标的系统层面得分
绘制了柱形图。
\input{data/barplot_group/BLEU.tex}
图~\ref{fig:BLEU_system}~是BLEU的系统得分柱形图。
BLEU-1和BLEU-2的情况比较类似：
从总体上看，模型在Ubuntu的得分普遍高于在另外两个数据集上的得分，
而HRED和VHRED在所有数据集上都比LSTM表现好，
在Ubuntu上，VHRED超过了HRED，但在另外两个数据集上，
HRED都一致的超过了VHRED。
模型的BLEU-1得分在虽然在不同数据集之间相差较大，
但在同一个数据集内却相差不大。
然而，模型的BLEU-2，BLEU-3，BLEU-4得分即便在同一个数据集内也相差很大。

BLEU-3和BLEU-4的情况比较类似：
从总体上看，没有哪一个数据集的得分要普遍高于其他数据集上的得分。
在所有数据集上，HRED和VHRED仍然普遍比LSTM得分高，
虽然有时候优势不太明显 。
VHRED在OpenSubtitles上大幅度的超过了HRED，
但在另外两个数据集上，HRED都超过了VHRED。
从绝对数量的角度来看，BLEU的得分随着N的增大而减小，
表现在图上就是，$N = i$的BLEU最低分不低于$N = i + 1$的最高分，
这可能是因为两个句子的n-gram共现数随着$N$的增大而减少。

\input{data/barplot_group/ADEM_EB.tex}
图~\ref{fig:ADEM_EB_system}~是基于词嵌入的指标和ADEM的系统得分柱形图。
有趣的是，ADEM对所有数据集上的所有模型的打分都非常接近2.6。
从表~\ref{tab:systemScoresAll}~看出，
当数据集不同时，ADEM的分数在百分位变化，
当数据集相同而模型不同时，ADEM的分数在千分位或之后变化，
这表明数据集对ADEM得分的影响要大于模型的影响。
\begin{table}[H]
    \centering
    \caption{ADEM的系统层面打分}
    \label{tab:ADEM_system}
    \begin{tabular}{llll}
        \toprule
        & HRED & LSTM & VHRED \\
        \midrule
        LSDSCC & 2.6178 & 2.6127 & 2.6163  \\
        OpenSubtitles & 2.6228 & 2.6224 & 2.6219 \\
        Ubuntu & 2.6353 & 2.6381 & 2.635 \\
        \bottomrule
    \end{tabular}
\end{table}
由于从图~\ref{subfig:ADEM_system}~中难以得分的优劣，
我们将从表~\ref{tab:ADEM_system}~得出结论：
首先，从数据集的角度，所有模型在Ubuntu上的得分好于在OpenSubtitles的得分，
而后者要好于在LSDSCC上的得分。
其次，从模型的角度，在LSDSCC上，HRED的得分好于VHRED的得分，
后者要好于LSTM的得分。
在OpenSubtitles上，HRED的得分好于LSTM的得分，
后者要好于VHRED的得分。
在Ubuntu上，最好的模型是LSTM，其次是HRED，最后是VHRED。
从上述分析来看，HRED在两个数据集上得分最优，
VHRED在两个数据集上得分最差，
LSTM在三个数据集上排名各不相同。
尽管得分的差异很小（最大值和最小值之间只相差0.019），
而且各个数据集上模型的排名都不相同，但是还是可以看出，
HRED在ADEM指标上表现较好，而VHRED则较差。

和ADEM相比，基于词嵌入的指标则表现的更具有一致性。
Average，Greedy和Extrema三种指标虽然在绝对数值上有些差异，
但是图像的模式却非常相似。

\input{data/barplot_group/ROUGE.tex}
\input{data/barplot_group/Other.tex}

\section{句子层面得分}\label{sec:utterance_scores}
我们还从句子层面得分方面进行了分析。
因为模型生成的句子$u$可以看做一个句子空间$U$上的随机变量，而指标是一个确定的函数$f_{s}$，
所以可以把句子层面的得分看做一个随机变量$\lambda_u = f_{s}(u)$。
于是我们便可以用描述统计学（Descriptive Statistics）和统计推断断（Statistical Inference）
的方法来分析指标在句子水平的情况，包括单变量和双变量的方法，以及各种统计数据。
以下所有图片均使用seaborn绘制\footnote{\url{http://seaborn.pydata.org/}}。

\subsection{指标的概率分布}\label{subsec:metric_distribution}
我们绘制了各种指标的句子层面得分的单变量分布（Univariate Distribution）。
为了便于从图像上比较各种指标的分布特征，我们对数值作了归一化（Scaling）处理，使数据的平均值为0，标准差为1：
\begin{align}
    x' = \frac{x - \mu}{\sigma}
\end{align}
我们绘制了所有模型和数据集的组合上的分布图。
图片的数量非常多，为了使读者能快速检阅所有指标的分布情况，
我们在正文只展示了模型为HRED，数据集为OpenSubtitles
的各项指标的分布。
请参阅附录~\ref{ch:metric_dist}~查看所有的组合上的分布。

\input{data/distplot_group/BLEU.tex}
图~\ref{fig:BLEU_dist}~是BLEU的N取1到4时的分布图。
由于加入了平滑处理，所以大部分得分都有非零数值，
然而正如\cite{HowNot}所说，大部分句子的得分非常低，
在图像上的反映是，在均值附近集中了大量的概率质量（Probability Mass）。
得分较高和较低的句子非常少，几乎没有。
比其更高的N值，Unigram匹配是最容易得到的，所以BLEU-1在
均值的右边有一个比较低矮的分布，并且峰值对应的横坐标偏离了均值0，
这说明有相当一部分句子的得分高于均值。
而低于均值的部分出现了一个高峰，说明也有大量句子得分低于均值，
但总体来说，得分高于均值的句子更多。
BLEU-2至4的分布都非常相似，峰值的横坐标比BLEU-1更接近均值。
从总体上来说，BLEU的分布形状十分尖锐，大量句子集中在均值或略高于均值附近，
图像不对称， 峰值比高斯分布的峰值$1 / \sqrt{2 \pi} \approx 0.4$高。

\input{data/distplot_group/ADEM_EB.tex}
图~\ref{fig:ADEM_EB_dist}~是基于词嵌入的指标和ADEM指标的分布图。
因为ADEM可以看做是一种更高级的词嵌入指标，所以把它和其他词嵌入指标并列。
四个图像均呈钟型曲线，峰值非常接近0.4，峰值的横坐标则非常接近0。
ADEM曲线显示出几乎完美的对称，Greedy曲线的对称度次之，
Average曲线向右边偏移，Extrema曲线向左边偏移。
ADEM和词嵌入指标有相似之处。
从句子向量的组合方式来看，
Average的组合方法是平均值，Extrema的组合方法是极端值，
而ADEM的则使用了预训练的VHRED的编码器。
与其他词嵌入指标不同的是，ADEM考虑了上下文的影响，
用神经网络对上下文，响应和参考三者加权，
并且显式的优化了和人类评价的相关性，
可见ADEM采用的机制更为复杂。
Lowe在\cite{ADEM}中指出，ADEM倾向于保守打分，
即人类评价给高分的句子，ADEM倾向于给稍低的分数。
从ADEM的曲线可以从某种意义上验证这一点，
得分高于平均值的句子与得分低于平均值的句子几乎一样多，
说明它不倾向于打高分或者低分。

\input{data/distplot_group/ROUGE.tex}
图~\ref{fig:ROUGE_N_dist}~和图~\ref{fig:ROUGE_LW_dist}~是ROUGE系列指标的分布图。
从曲线的形状来看，大致可分为两类：
ROUGE-1，ROUGE-L，ROUGE-W是一类，
ROUGE-2，ROUGE-3，ROUGE-4是一类。
ROUGE-2这类图像的特点是几乎所有句子的得分都集中在均值，
除此之外没有其他得分。
因为我们没有对ROUGE指标做任何平滑处理，
我们猜想原因是除了极个别句子外，其他句子都得了零分。
这表明ROUGE-N指标当$N > 1$时捕捉不到任何n-gram重叠。
与BLEU不同的是，ROUGE-N不考虑$n < N$的其他n-gram，
导致它更容易给出0分。

另一方面，ROUGE-1类指标更像是BLEU-1指标的噪化版本，
它们都是双峰曲线，而且第一个峰值对应的点$(x_1, y_1)$和
第二个峰值对应的点$(x_2, y_2)$都有相似的坐标。
读者可以验证以下取值范围对BLEU-1，ROUGE-1，ROUGE-L，ROUGE-W都成立：
\begin{align}
    -1 < x_1 < 0, 1 < y_1 < 1.5 \\
    0 < x_2 < 2, 0 < y_2 < 0.5
\end{align}

造成上述现象的原因是，所有的响应和参考基本上没有$N > 1$的n-gram重叠，
因此基于最长公共子序列和它的加权版本的匹配都退化成了Unigram匹配，
相应的，ROUGE-1，ROUGE-L，ROUGE-W都退化成了BLEU-1。

\input{data/distplot_group/Other.tex}
图~\ref{fig:Other_dist}~是Distinct-N，METEOR和\#words的分布图。
我们发现Distinct-1的分布呈现两极分化，
在均值附近和均值右边较远处都聚集了大量句子。
Distinct-N的分母是句子的长度（\#words），
分子是句子中各异的n-gram数量。
从\ref{fig:words_dist}来看，句子的长度集中在均值附近，
长于均值的句子比短于均值的句子多，
这表明Distinct-N的值主要受分子影响。
Distinct-1的曲线表明，极大部分句子的各异的单词数量都接近平均值，
但也有少部分句子各异的单词数量远远小于平均值。
% -- Distinct-2
Distinct-2的图像没有出现两极分化，
可能是因为Bigram的空间比Unigram更大，
一个句子更容易出现各异的Bigram。
图像的特点是：
\begin{enumerate}
    \item 大部分句子集中在高于均值的一片区域。
    \item 个别低于均值的区间聚集了大量句子。
\end{enumerate}
这可以理解为模型在应对不同的消息时，
生成的句子质量不一，对一部分消息产生了平庸的响应，
对另一部分消息却产生了单调的响应。

尽管通常被分类成基于词重叠的指标，
METEOR的图像表现出了和其他基于词重叠的指标（BLEU，ROUGE）相当不同的性质。
它似乎能区分不同响应的质量，并且和句子长度有着某种联系。
图像在均值的右侧表现出指数衰减， 在左侧则是一个尖锐的单峰。
本质上，METEOR是基于Unigram匹配的，
但是我们尚不清楚METEOR的多种匹配模块，对齐算法和惩罚系数是如何
使它的分布变得与BLEU-1等指标的分布如此不同。

% -- Conclusion -- %
中心极限定理（Central Limit Theorems）表明：大量独立随机变量的叠加近似于高斯分布。
我们假设当句子足够多时，句子层面的人类评价近似于高斯分布。
据此我们认为，句子层面得分的分布接近高斯分布是一个指标和人类评价具有较高相关性的前提条件之一。
从上述各个指标的分布来看， ADEM和词嵌入指标具有很大的潜力。

最后要提醒读者，
本节的分析结果是从OpenSubtitles和HRED的组合中得出的，
并不能完全代表所有组合下的指标分布情况。
观察附录~\ref{ch:metric_dist}可发现， 一般来说，
指标的分布当模型不同而数据集相同时比较相似，
当模型相同而数据集不同时则有较大差异，
因此句子层面的指标分布受数据集的影响较大。

\subsection{不同指标的相关性分析}
\label{subsec:metric_correlation}

\section{定性分析}\label{sec:qualitative_analysis}

\section{结果与讨论}\label{sec:result_and_discussion}

\section{本章小结}\label{sec:experiment_conclusion}
