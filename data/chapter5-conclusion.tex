% !Mode:: "TeX:UTF-8"
% Author: Zhengxi Tian
% Email: zhengxi.tian@hotmail.com

\chapter*{结论\markboth{结论}{}}\label{ch:conclusion}
\addcontentsline{toc}{chapter}{结论}

\section{总结}\label{sec:conclusion}
% -- Related Work -- %
本文对生成式对话领域的模型，数据集和指标进行了一次深入考察。
我们首先介绍了生成式对话领域的兴起过程中一些重要的模型，比如
Ritter等人的SMT模型\upcite{Ritter11}，
Sordoni等人的DCGM模型\upcite{DCGM}和Vinyals等人的
NCM模型\upcite{GoogleChatbot}。
接着，在相关工作中，我们介绍了生成式模型的定义，
模型的核心组件RNN，以及几种常见的生成式模型，包括最简单的RNN语言模型，
广为流行的Seq2Seq框架和多层编解码器HRED。
我们还介绍了本领域用到过的一些指标，包括基于词重叠的BLEU，
ROUGE和METEOR，基于词嵌入的Average，Extrema和Greedy，
衡量概率语言模型性能的困惑度，以及专门为生成式对话设计的ADEM和RUBER。
最后，我们介绍了生成式对话文献中常见的数据集，
包括Twitter Dialogue Corpus，Ubuntu Dialogue Corpus和
OpenSubtitles等等。

% -- Method -- %
在众多模型，数据集和指标中，我们选择了Serban等人在\cite{VHRED}
中使用的三个模型HRED，LSTM和VHRED。
这三个模型有着成熟的实现，易于其他人复现我们的实验。
在数据集方面，我们选择了公开且易得的，代表了不同领域的三个数据集，
分别是Ubuntu Dialogue Corpus，OpenSubtitles和LSDSCC。
这些数据集的领域都是文献中常见的。
在指标方面，我们尽可能涵盖了对话领域使用过或者提出的指标，
在配置方面与\cite{HowNot}大致对齐。
实验的主要工作主要是：
\begin{enumerate}
    \item 在多个数据集上训练多个模型。
    \item 在训练结果上运行多个评价指标。
    \item 对指标进行多种数据分析。
\end{enumerate}
我们的实验数据是由多个模型和数据集的组合在不同指标上的系统层面和句子层面
得分，以及这些模型输出的响应组成的。
由于时间关系，我们只分析了得分的数据。

我们探索了系统层面和句子层面的实验结果。
系统层面得分是是对一个模型在一个数据集上的表现的粗粒度考察，
它可能掩盖了一些事实，
但是方便我们综合考察模型、数据集和指标三者的整体关系。
我们参考\cite{HowNot}，将指标的数据进行分组，
并以此组织所有的分析。我们把BLEU的N取不同值的指标分为一组，
把ROUGE的所有变形分为一组，把词嵌入的指标和ADEM分为一组，
把剩下的METEOR，Distinct-N和句子长度归到一组。
分组的最初目的是便于合理利用有限的页面空间，
但是它也让数据分析更加有条理。

各个模型的系统层面得分随着数据集的变化和指标的变化有较大差异，
比较稳定的是同一个数据集上各个模型的排名，
一般HRED和VHRED比LSTM优秀，
然而有时优势不太明显，有时会出现LSTM反超HRED和VHRED的情况。
模型在Ubuntu Dialogue Corpus上的各项指标通常更好，
但是有时候另外两个数据集的某些得分会反超。
从附录~\ref{ch:dataset_system_dist}~和附录~\ref{ch:model_system_dist}~可以发现，数据集和模型对得分都有影响，
从大体上看，HRED和VHRED优于LSTM，Ubuntu Dialogue Corpus的得分
高于其他数据集，但是这不是绝对的，实验数据中存在许多反例。
我们认为，除了实验过程本身带来的噪音外，
主要的原因是数据集的特征变化太大，
模型无法完全将一个数据集上的性能迁移到另一个数据集上。
而且，
实验中的指标对生成式对话的适用程度不同，
指标形成了集群现象，不同集群之间一致性很低，
给评价造成了混乱。

我们在句子层面的分析主要采取得分的单变量概率分布的形式，
从不同指标的分布图像中验证了指标之间的集群现象。
指标的集群反映了提取相同特征的指标很可能有相似的行为，
不管它们如何使用这些特征。
在这些指标的集群中，我们发现图像大致有两类：
\begin{enumerate}
    \item 基于词嵌入的指标的图像接近高斯分布。
    \item 基于词重叠的指标的图像是不对称的双峰曲线，
    大量句子集中在均值附近的很小范围。
\end{enumerate}
我们假设在大量句子上的人类评价将接近高斯分布，
从而认为分布接近高斯分布的指标是更好的选择。
我们基于句子得分分布的分析发现了指标在大量句子上的统计规律，
从统计的角度验证了词嵌入指标比词重叠指标更适合本领域\upcite{HowNot}。

我们的实验有些不完善的地方。
所有数据集上的LSTM模型的门单元应该统一使用LSTM；
解码时我们使用了随机取样，结果发现响应中有很多语法错误，
使用集束搜索可以产生语法错误较少的响应。
我们没有针对特定数据集做参数的调优，
导致模型在有些数据集上的表现没有达到最优。
我们也没有像\cite{VHRED}那样用预训练的HRED初始化VHRED的参数，
结果VHRED的性能没有达到最优。
我们将汲取经验教训，尽量减少实验程序带来的噪音。

\section{展望}\label{sec:future_work}
% -- simpily CheDan or NaoDong -- %
在实验结论的基础上，我们对生成式对话领域的模型，数据集以及指标提出几点展望。

% -- Model -- %
在模型方面，两个可能的思路是：
\begin{enumerate}
    \item 使用新的模型体系结构。
    \item 加入更多特征。
\end{enumerate}
尽管Seq2Seq框架在机器翻译领域取得成功，
但是在更加困难的对话生成领域，它的表达力可能遇到了瓶颈。
Seq2Seq框架本质上是一种带注意力的基于RNN的编解码器结构，
我们可以尝试其他编解码器结构，比如Transformer\upcite{Transformer}。
我们还可以尝试对抗生产网络和强化学习，
正如Li等人在\cite{deep_RL,Adversarial}所做的那样。
另一方面，我们可以加入感情色彩\upcite{ECM}，
主题词\upcite{Topic_Aware}以及对话者身份信息\upcite{persona}
等特征， 使模型的输出带有上述特征，从而更加符合人类评价。

% -- Dataset -- %
在数据集方面，我们认为通过互联网收集的数据集容易引入多方面的噪音。
事实上，这些数据集的特征和人类在互联网这种匿名平台的表现有密切联系。
由于在这种平台上人们的言论没有什么限制，对话的话题非常多样，
而且语言风格，语法习惯和感情色彩和具体的用户和具体的对话有关，
这就使得数据集具有很高的熵。
从概率的角度，数据集的样本分布可以看作是非常多个随机变量的叠加，
如果这些随机变量都是独立同分布
（Independently Identical Distributed，IID）的话，
整个数据集的样本分布就趋向于高斯分布。
在实验中，我们发现基于词嵌入的指标在大量模型
响应上的分布和高斯分布很接近，
我们当时给出的解释是接近高斯分布是一个指标
和人类评价具有相关性的前提条件。
但是，因为模型的响应的分布在一定程度上反映了数据集的样本分布，
假设基于词嵌入的指标能反映两个句子的话题相关性，
那么对它们的分布接近高斯分布的另一种解释就是，
数据集的话题分布接近高斯分布，
而高斯分布是$(-\infty, +\infty)$上方差已知的连续分布中，
熵最大的分布。
面对这样复杂的数据集，我们应该用概率统计学的工具详细分析它在
多个方面的分布情况，例如情感分布，对话轮数分布等等，
这样有助于我们理解在这个数据集上训练模型的难度。

% -- Metric -- %
在指标方面，我们的实验大体上验证了\cite{HowNot}的结论。
由于ADEM指标在Twitter数据集上和人类评价有很高的相关性，
虽然不知道它的性能是否可以迁移到其他数据集，
但是如果把ADEM作为其他指标的黄金标准的话，
从句子得分分布上，
我们就发现基于词重叠的指标和ADEM的相关性较差，
而基于词嵌入的指标和ADEM的相关性则较好。
这表明在目前已有的指标中，
基于词嵌入的指标可以作为构建更高的指标的基础。
我们认为，生成式对话的评价指标首先要解决一个重要的问题，
即，什么样的对话才是好的对话。
这个问题之所以重要，是因为不仅指标的构建依赖于对它的回答，
甚至模型的构建也需要参考它的答案。
我们不认为这个问题的答案可以由“语义相关性高”或者“单词多样性高”
这些片面的特征来回答，尽管它们可能是答案的一部分。
这个问题可能难以回答，甚至没有普适性的答案，
因为人类的对话是在自然环境和社会环境中演化出来的，
根据不同场合和参与者而变化的一种语言现象，非常复杂。

从实用的角度，我们可以通过实验发现在某些数据集上好的对话应有的特征，
比如在Ubuntu Dialogue Corpus上，好的对话应该和涉及到的技术话题
有较高的相关性，因为这个数据集上的对话以“提问-解决问题”为主，
好的对话应该能帮助人们解决问题；
而在Twitter Dialogue Corpus上，好的对话应该考虑情感因素，
关注主题的同时又具有一定的多样性，因为人们在Twitter上主要发布个人的状态信息，
经常带有感情色彩，而且期望从响应中看到相同的话题或者新奇的事物；
在LSDSCC上，好的对话应该能对相关的电影发表中肯的评价，
因为在Reddit的电影板块上，人们主要发表对电影的点评，
希望和看过相同或者类似电影的人一起讨论，人们虽然有时候会发表极端的评价，
但是人们并不希望总是看到极端的评价，所以模型的评价应该中肯，
而且带有一点个性化看法。
虽然对于“什么样的对话才是好的对话”这个问题无法找到答案，
但是我们不妨对它加上“在某个数据集上”的限定词，
从某个数据集中发现最受欢迎的对话的模式，然后设计指标捕获这些模式的特征。

基于上述思考，我们可能需要承认无法找到一般意义下好的对话意味着什么，
但是却能对具体的数据集回答这个问题。
这种思路或许能为生成式对话领域带来一个新的范式：
\begin{enumerate}
    \item 首先我们建立一个开放式领域对话数据集，并通过人类调查，
    发现“好的对话”在这个数据集上应该具有什么性质。
    \item 接着，我们设计出一系列能够衡量所谓的“好的性质”的指标，
    并确保它们和人类评价的相关性。
    \item 我们把该数据集和与之匹配的评价指标称为一个“问题”（Problem），
    让不同的模型去解决这个问题。
\end{enumerate}

进一步，我们希望把“设计出和人类评价相关度高的指标”这一个任务分解为若干小的任务：
\begin{enumerate}
    \item 把问题限制在某个数据集上，并且承认不同数据集上答案没有必然联系。
    \item 找出这个数据集上人类评价高的对话具有的特征。
    \item 设计出能准确捕获这些特征的指标。
    \item 用人类评价验证上述指标在上述数据集上的有效性。
\end{enumerate}

必须指出的是，第二步和第三步具有很大的难度。
第二步一般需要人类评价员对数据集的样本打分然后再排序，
导致能够评价的样本数量非常受限，这对指标的泛化能力提出了很高的要求\upcite{ADEM}。
第三步为了减少写死的特征（Hard-coded Feature），建议使用神经网络实现。


%对于在开放领域对话数据集上训练模型，存在两种看法：
%第一种看法认为，模型能够自动从充满噪音的数据集中学习到常识
%（Common Knowledge）\upcite{GoogleChatbot}，
%另一种看法认为，因为对话建模本身就充满了歧义性，
%即便是在训练集上，对同一个消息也有多个响应，模型学到的只是这些响应的
%平均值。
