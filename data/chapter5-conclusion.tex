\chapter*{结论\markboth{结论}{}}\label{ch:conclusion}
\addcontentsline{toc}{chapter}{结论}

\section{总结}\label{sec:conclusion}
% -- Related Work -- %
本文对生成式对话领域的模型，数据集和指标进行了一次深入考察。
我们首先介绍了生成式对话领域的兴起过程中一些重要的模型，比如
Ritter等人的SMT模型\upcite{Ritter11}，
Sordoni等人的DCGM模型\upcite{DCGM}和Vinyals等人的
NCM模型\upcite{GoogleChatbot}。
接着，在相关工作中，我们介绍了生成式模型的定义，
模型的核心组件RNN，以及几种常见的生成式模型，包括最简单的RNN语言模型，
广为流行的Seq2Seq框架和多层编解码器HRED。
我们还介绍了本领域用到过的一些指标，包括基于词重叠的BLEU，
ROUGE和METEOR，基于词嵌入的Average，Extrema和Greedy，
衡量概率语言模型性能的困惑度，以及专门为生成式对话设计的ADEM和RUBER。
最后，我们介绍了生成式对话文献中常见的数据集，
包括Twitter Dialogue Corpus，Ubuntu Dialogue Corpus和
OpenSubtitles等等。

% -- Method -- %
在众多模型，数据集和指标中，我们选择了Serban等人在\upcite{VHRED}
中使用的三个模型HRED，LSTM和VHRED。
这三个模型有着成熟的实现，易于其他人复现我们的实验。
在数据集方面，我们选择了公开的，代表了不同领域的三个数据集，
分别是Ubuntu Dialogue Corpus，OpenSubtitles和LSDSCC。
%这些数据集的领域都是文献中常见的。
在指标方面，我们尽可能涵盖了对话领域使用过或者提出的指标，
在配置方面与\upcite{HowNot}大致对齐。
实验的主要工作是：
\begin{enumerate}
    \item 在多个数据集上训练多个模型。
    \item 在训练结果上运行多个评价指标。
    \item 对指标进行多种数据分析。
\end{enumerate}
我们的实验数据是多个模型和数据集的组合在不同指标上的得分，
以及这些模型输出的响应。
由于时间关系，我们只分析了得分的数据。

我们探索了系统层面和句子层面的得分。
系统层面得分是是对一个模型在一个数据集上的表现的粗粒度考察，
它可能掩盖了一些事实，
但是方便我们综合考察模型、数据集和指标三者的整体关系。
我们参考\upcite{HowNot}，将指标的数据进行分组，
并以此组织所有的分析。我们把BLEU的N取不同值的指标分为一组，
把ROUGE的所有变形分为一组，把词嵌入的指标和ADEM分为一组，
把剩下的METEOR，Distinct-N和句子长度归到一组。

各个模型的系统层面得分随着数据集的变化和指标的变化有较大差异，
比较稳定的是同一个数据集和指标上各个模型的排名，
一般HRED和VHRED比LSTM好，
有时优势不太明显，有时会出现LSTM反超的情况。
模型在Ubuntu Dialogue Corpus上的各项指标通常更好，
但是有时候某些指标在另外两个数据集上的得分会反超。
从附录~\ref{ch:dataset_system_dist}~可以发现，
数据集和模型对得分都有影响，
从大体上看，HRED和VHRED优于LSTM，Ubuntu Dialogue Corpus的得分
高于其他数据集，但是这不是绝对的，实验数据中存在许多反例。
我们认为，除了实验过程本身带来的噪音外，
主要的原因是数据集的特征变化太大，
模型无法完全将一个数据集上的性能迁移到另一个数据集上。
而且，实验中的指标形成了集群现象，不同集群之间一致性很低，
给评价造成了混乱。

我们在句子层面的分析主要采取单变量概率分布的形式，
从不同指标的分布图像中验证了指标之间的集群现象。
指标的集群反映了提取相同特征的指标很可能有相似的行为，
不管它们如何使用这些特征。
在这些指标的集群中，我们发现图像大致有两类：
\begin{enumerate}
    \item 基于词嵌入的指标的图像接近高斯分布。
    \item 基于词重叠的指标的图像是不对称的双峰曲线， 大量句子集中在均值附近很小的范围。
\end{enumerate}
我们假设在大量句子上的人类评价将接近高斯分布，
从而认为分布接近高斯分布的指标是更好的选择。
我们发现了指标在大量句子上的统计规律，
从统计的角度指出词嵌入指标更适合本领域。

我们的实验有些不完善的地方。
所有数据集上的LSTM模型的门单元应该统一使用LSTM；
解码时我们使用了随机取样，结果发现响应中有很多语法错误，
而使用集束搜索可以产生语法错误较少的响应。
我们没有针对特定数据集做参数的调优，
导致模型在有些数据集上的表现没有达到最优。
我们也没有像\upcite{VHRED}那样用预训练的HRED初始化VHRED的参数，
结果VHRED的性能没有达到最优。
LSDSCC的测试集样本数与其他两个数据集的不成比例，
导致在LSDSCC上的实验可能受数据稀疏性影响较大。
我们将汲取经验教训，尽量减少实验程序带来的噪音。

\section{展望}\label{sec:future_work}
在实验结论的基础上，
我们对生成式对话领域的模型，数据集以及指标提出几点展望。
% -- Model -- %
在模型方面，我们提出两个可能的思路是：
\begin{enumerate}
    \item 使用新的模型体系结构。
    \item 加入更多特征。
\end{enumerate}

尽管Seq2Seq框架在机器翻译领域取得成功，
但是在更加困难的对话生成领域，它的表达力可能遇到了瓶颈。
Seq2Seq框架本质上是一种带注意力的基于RNN的编解码器结构，
我们可以尝试其他编解码器结构，
比如Transformer\upcite{Transformer}。
我们还可以尝试对抗生产网络和强化学习，
正如Li等人在\upcite{deep_RL,Adversarial}所做的那样。
另一方面，我们可以加入感情色彩\upcite{ECM}，
主题词\upcite{Topic_Aware}以及
对话者身份信息\upcite{persona}
等特征，使模型的输出带有上述特征，从而更加符合人类评价。

% -- Dataset -- %
在数据集方面，
我们认为通过互联网收集的数据集容易引入多方面的噪音。
事实上，
这些数据集的特点和人类在互联网这种匿名平台的表现有密切联系。
由于在这种平台上人们的言论没有什么限制，对话的话题非常多样，
而且语言风格，语法习惯和感情色彩和具体的用户与具体的对话有关，
这就使得数据集具有很高的熵。
从概率的角度，
数据集的样本分布可以看作是非常多个随机变量的叠加，
如果这些随机变量都是独立同分布
（Independently Identical Distributed）的话，
整个数据集的样本分布就趋向于高斯分布。

在实验中，我们发现基于词嵌入的指标在大量模型
响应上的分布和高斯分布很接近，
我们当时给出的解释是，
接近高斯分布是一个指标和人类评价具有相关性的前提条件。
但是，因为模型的响应的分布在一定程度上反映了数据集的样本分布，
假设基于词嵌入的指标能反映两个句子的话题相关性，
那么对它们的分布接近高斯分布的另一种解释就是，
数据集的话题分布接近高斯分布。
高斯分布是$(-\infty, +\infty)$
上方差已知的连续分布中熵最大的分布，
面对这样复杂的数据集，我们应该用概率统计学的工具详细分析它在
多个方面的分布情况，例如情感分布，对话轮数分布等等，
这样有助于我们理解在这个数据集上训练模型的难度。

% -- Metric -- %
在指标方面，我们的实验大体上验证了\upcite{HowNot}的结论。
我们认为生成式对话的评价指标首先要解决一个重要的问题，
即什么样的对话才是好的对话。
这个问题之所以重要，是因为它不仅能指导指标的构建，
还决定了模型优化的方向。
我们认为这个问题的答案不是“语义相关性高”或者“n-gram多样性高”
这些片面的特征，尽管它们可能是答案的一部分。
这个问题可能难以回答，甚至没有普适性的答案，
因为人类的对话是在自然环境和社会环境中演化出来的一种语言现象，
它根据不同场合和参与者的变化而变化的，非常复杂。

从实用的角度，
我们可以通过实验发现在某些数据集上好的对话应有的特征。
直观的来说，
在Ubuntu Dialogue Corpus上，好的对话应该和具体的技术话题
有较高的相关性，因为这个数据集上的对话以“提问-解决问题”为主，
好的对话应该能帮助人们解决问题；
而在Twitter Dialogue Corpus上，好的对话应该考虑情感因素，
关注主题的同时又具有一定的多样性，
因为人们在Twitter上主要发布个人的状态信息，
经常带有感情色彩，
而且期望从响应中看到相同的话题或者新奇的事物；
在LSDSCC上，好的对话应该能对特定的电影发表中肯的评价，
因为在Reddit的电影板块上，人们主要发表对电影的点评，
希望和看过相同或者类似电影的人一起讨论，
人们虽然有时候会发表极端的评价，
但是不希望总是看到极端的评价，
所以模型的评价应该中肯，
最好带有一点个性化看法。
虽然“什么样的对话才是好的对话”这个问题很难回答，
但是我们不妨对它加上“在某个数据集上”的限定词，
从某个数据集中发现最受欢迎的对话的模式，
然后设计出能捕获这些模式的指标，
并用它一致的评价在该数据集上训练的模型的表现。
这个思路或许能为生成式对话领域带来一个新的范式：
\begin{enumerate}
    \item 首先构建一个开放式领域对话数据集，并
    发现“好的对话”在这个数据集上应该具有什么性质。
    \item 接着，设计出一系列能够衡量所谓的“好的性质”的指标，
    并确保它们和人类评价具有一致性。
    \item 把数据集和与之匹配的指标称为一个“问题”（Problem），
    让不同的模型去解决这个问题。
\end{enumerate}

进一步，我们希望把“设计出和人类评价相关度高的指标”
这一个任务分解为若干个小任务：
\begin{enumerate}
    \item 把问题限制在某个数据集上。
    \item 找出这个数据集上人类评价高的对话具有的特征。
    \item 设计出能准确捕获这些特征的指标。
    \item 用人类评价验证指标在对应数据集上的有效性。
\end{enumerate}

必须指出的是，第二步和第三步具有很大的难度。
第二步一般需要人类评价员对数据集的样本打分，
这导致带有人类评价的样本数量非常受限，
这对指标的泛化能力提出了很高的要求\upcite{ADEM}。
第三步涉及的特征比较抽象，对指标的建模能力提出了很高的要求。

最后，我们来谈谈生成式对话和人的关系，以及这关系背后的意义。
面向任务的对话系统的功能是用对话的形式帮助人完成任务。
而生成式对话系统的功能是娱乐、语言学习和陪伴，这些功能其实不乏替代品：
现代人从来不缺少娱乐，语言学习也有成熟的产业链，
而亲人和爱人的陪伴是公认的最好的陪伴。
生成式系统要如何在竞争中受到人们的青睐呢？
首先，我们要认清生成式系统的功能的本质，
就是要让人类在和机器的对话中有所收获，比如
让人们感受到聊天的快乐，
在人们失落时送去慰藉，
在讨论中激发人们的灵感等等，
而不仅仅是打发无聊的时间。

虽然这个目标对于现在的系统来说有些遥远，
但是为了实现它，我们需要把人类的需求考虑进来。
人们在对话方面的需求是多种多样的，
有的人通过对话获取信息，
有的人通过对话排解情绪，
有的人只是想寻找快乐。
虽然说需求分析比较接近应用而偏离理论，
但是应用和理论相结合能增加研究成果的实用性，
使研究本身从应用所获取的反馈中受益。
因此我们建议把最大化人类需求的满足程度作为模型的目标函数之一，
并设计相关的指标加以衡量。
我们相信考虑了人类需求的模型将更受人类评价的青睐。
